{"name":"Raft Lib","tagline":"Simple, easy to use stream computation library for C++.","body":"### Welcome to the Raft C++ template library\r\n\r\n### Current Status\r\nThe Raft library is currently an alpha release.  The API itself is stable, however the back end\r\nis subject to updates.  The release is subject to the Apache Version 2.0 license.\r\n\r\n### Setup\r\nTo checkout the library do the following:\r\n```\r\n$ cd location_for_the_repo\r\n$ git clone git@github.com:jonathan-beard/RaftLib.git\r\n$ cd RaftLib\r\n$ make\r\n$ sudo make install\r\n```\r\n\r\nThis will generate the static library file _libraft.a_.  When building an application\r\nusing the library the only header file needed is the _raft_ header.  Once the library\r\nis build you're basically set save for a few details, which will be laid out within the tutorial\r\n as we go through the example we're about to build.\r\n\r\n\r\nTo build the example application simply:\r\n```\r\n$ cd RaftLib/ExampleApps\r\n$ make\r\n```\r\n\r\n\r\n\r\n### Authors and Contributors\r\nIn 2013 Jonathan Beard (@jonathan-beard) started work on the Raft language.  In the interim,\r\nand realizing the ubiquity of the C++ language, he started work on a template library that \r\nuses the same framework as the full Raft language and run-time system.  This C++ library is \r\nthe result.\r\n\r\n### Genealogy\r\nThere have been several dozen streaming languages.  Most notable of which is probably StreaMIT.  I've worked on the Auto-Pipe streaming runtime system (both versions 1 and 2) developed at Washington University.  This particular streaming library was developed as a faster way to get started with stream processing by enabling users to stick to a language with which they are already familiar (C++).\r\n\r\n### Tutorial\r\nThe tutorial could be written two ways.  The first would be to enumerate all the possible functions and configurations.  The second would be to give an example and explain the example as it is presented.  We'll start with the example approach then move to a short directory of useful functions followed by some permutations on the previously mentioned example.\r\n\r\nFirst and foremost what is stream processing?  Essentially it is a compute paradigm that envisions perfectly safe, threaded programs (although we'll relax this a bit with locked shared objects, shown much later in the tutorial).  Stream programs themselves are decomposed into compute \"kernels\" which are connected by communications links called \"streams.\"  As an extremely simple example, take a look at the figure below which has three compute \"kernels\" (A,B,C) connected by two \"streams.\"\r\n\r\n![tandem queue](http://www.cse.wustl.edu/~beardj/queues.jpg)\r\n\r\nAs a concrete example we have a \"sum\" application which generates a random stream of numbers from two separate threads, sums them in another and finally prints them in yet another thread.  Admittedly this is a bit wasteful in practice, however its a nice dirt simple example. \r\n\r\n```c++\r\n#include <cassert>\r\n#include <iostream>\r\n#include <cstdint>\r\n#include <cstdlib>\r\n/** include the raft header file **/\r\n#include <raft>\r\n\r\n\r\ntemplate < typename T > class Generate : public Kernel\r\n{\r\npublic:\r\n   Generate( std::int64_t count = 1000 ) : Kernel(),\r\n                                          count( count )\r\n   {\r\n      output.addPort< T >( \"number_stream\" );\r\n   }\r\n\r\n   virtual raft::kstatus run()\r\n   {\r\n      if( count-- > 1 )\r\n      {\r\n         output[ \"number_stream\" ].push( count );\r\n         return( raft::proceed );\r\n      }\r\n      output[ \"number_stream\" ].push( count, raft::eof );\r\n      return( raft::stop );\r\n   }\r\n\r\nprivate:\r\n   std::int64_t count;\r\n};\r\n\r\ntemplate< typename A, typename B, typename C > class Sum : public Kernel\r\n{\r\npublic:\r\n   Sum() : Kernel()\r\n   {\r\n      input.addPort< A >( \"input_a\" );\r\n      input.addPort< B >( \"input_b\" );\r\n      output.addPort< C  >( \"sum\" );\r\n   }\r\n   \r\n   virtual raft::kstatus run()\r\n   {\r\n      A a;\r\n      B b;\r\n      raft::signal  sig_a( raft::none  ), sig_b( raft::none );\r\n      input[ \"input_a\" ].pop( a, &sig_a );\r\n      input[ \"input_b\" ].pop( b, &sig_b );\r\n      assert( sig_a == sig_b );\r\n      C c( a + b );\r\n      output[ \"sum\" ].push( c , sig_a );\r\n      if( sig_b == raft::eof )\r\n      {\r\n         return( raft::stop );\r\n      }\r\n      return( raft::proceed );\r\n   }\r\n\r\n};\r\n\r\ntemplate< typename T > class Print : public Kernel\r\n{\r\npublic:\r\n   Print() : Kernel()\r\n   {\r\n      input.addPort< T >( \"in\" );\r\n   }\r\n\r\n   virtual raft::kstatus run()\r\n   {\r\n      T data;\r\n      raft::signal  signal( raft::none );\r\n      input[ \"in\" ].pop( data, &signal );\r\n      fprintf( stderr, \"%\" PRIu64 \"\\n\", data );\r\n      if( signal == raft::eof )\r\n      {\r\n         return( raft::stop );\r\n      }\r\n      return( raft::proceed );\r\n   }\r\n};\r\n\r\nint\r\nmain( int argc, char **argv )\r\n{\r\n   Map map;\r\n   auto linked_kernels( map.link( new Generate< std::int64_t >(),\r\n                                  new Sum< std::int64_t,std::int64_t, std::int64_t >(),\r\n                                  \"input_a\" ) );\r\n   map.link( new Generate< std::int64_t >(), &( linked_kernels.dst ), \"input_b\" );\r\n   map.link( &( linked_kernels.dst ), new Print< std::int64_t >() );\r\n   map.exe();\r\n   return( EXIT_SUCCESS );\r\n}\r\n```\r\n\r\nThe first thing to notice is that all compute kernels extend the class Kernel.  This sets up the \"ports\"\r\nthat exist for communication between compute kernels.  For each kernel there are multiple ports.  Input\r\nports and output ports for each kernel are maintained in a data structure of that name.  When designing \r\na new kernel, the constructor must declare all ports as such:\r\n\r\n```c++\r\n   [input | output].addPort< [type] >( [port name] );\r\n```\r\n\r\nThis has two effects, the first registering a type with the port to ensure type safety, and the second\r\nregistering a port with a name so that it can be quickly accessed later.  These ports are constructed\r\nas lock free first in, first out buffers or \"ring-buffers\" which enable communication.  The run-time\r\nsystem decides on how big the buffer should be and has the option of dynamically monitoring the buffer\r\nto adjust the size at run-time.  Once ports are declared within the constructor the second function that\r\nmust be declared is _run()_.  It performs the main work of the kernel, although sub-functions can be \r\ndeclared to compartmentalize code.  As an example:\r\n\r\n```c++\r\n   virtual raft::kstatus run()\r\n   {\r\n      if( [ some bool here ] )\r\n      {\r\n         output[ \"the_port\" ].push( count );\r\n         /** return raft::proceed to get called again **/\r\n         return( raft::proceed );\r\n      }\r\n      /** else **/\r\n      /** \r\n       * call push with the last item and send a signal that will terminate\r\n       * the application such as raft::eof\r\n       */\r\n      output[ \"the_port\" ].push( count, raft::eof );\r\n      /** return raft::stop to end the calling of this kernel **/\r\n      return( raft::stop );\r\n   }\r\n```\r\n\r\nThe run function can be executed in a separate thread or in it's own process depending \r\non how the scheduler schedules it.  In either case, it will be called until it returns\r\nraft::stop, otherwise raft::proceed should be returned.  Otherwise there is nothing\r\nreally special about the Kernel class.  In future tutorials I'll add an example of how\r\nthe Raft library system an be used with OpenCL with the queue talking directly to\r\na kernel running on a GPU.  GPU kernels will have a slightly differing structure, but \r\nfrom the programmers perspective will have similar port semantics as the C++ kernels.\r\n\r\nWithin the kernel (not just the run function), there are multiple ways to access the input / output\r\nports including the following functions:\r\n\r\n```c++\r\n/**\r\n * allocate - returns a reference to the current tail \r\n * of the queue.  Enables the programmer to perform a\r\n * single copy (i.e., allocating memory directly on the\r\n * stream to use and then release to the downstream \r\n * kernel with the push(signal) function below.\r\n */\r\ntemplate < class T > auto allocate() ->T&;\r\n\r\n/**\r\n * allocate_range - similar to the single allocate above, allows the programmer\r\n * to allocate multiple items on the queue.  Returns a std::vector of \r\n * references.  Currently the \"auto\" keyword should be avoided when putting\r\n * this in a for loop, the type contained T should be used as a reference to \r\n * write to these values.  The first param is the number that the user wishes\r\n * to allocate, the second n_ret is set to the value actually allocated.  n_ret\r\n * will only differ if the capacity of the queue is less than n. \r\n */\r\ntemplate < class T > auto allocate_range( const std::size_t n, std::size_t &n_ret ) -> std::vector< std::reference_wrapper< T > >;\r\n\r\n/**\r\n * push - used after a call to allocate to release these values\r\n * to the stream.  The signal passed is assigned to the last value\r\n * from the allocate call so that it is received inline.\r\n */\r\nvirtual void push( const raft::signal = raft::none );\r\n\r\n/**\r\n * push - this version takes a reference (item) which is then copied to \r\n * the current position within the queue.  The signal, if given is \r\n * assigned to that index.\r\n */\r\ntemplate < class T > \r\n   void push( T &item, const raft::signal signal = raft::none );\r\n\r\n/**\r\n * insert - pass an iterator range to the queue, the signal\r\n * if used will assign the signal to the last position (i.e.,\r\n * end() - 1. \r\n */\r\ntemplate< class iterator_type >\r\n   void insert(   iterator_type begin,\r\n                  iterator_type end,\r\n                  const raft::signal signal = raft::none );\r\n\r\n/**\r\n * pop - pass a reference to an item of type T and the value at\r\n * the head of the queue will be copied to item and the signal\r\n * associated with that position copied.  The item is removed \r\n * from the queue after this call.\r\n */\r\ntemplate< class T >\r\n   void pop( T &item, raft::signal *signal = nullptr );\r\n\r\n/** \r\n * peek - returns the item at the head of the queue with\r\n * type T& but DOES NOT pop the item.  The signal associated\r\n * with this item is returned if it is desired.  A call to \r\n * recycle() after this function will remove the item previously\r\n * returned by peek().\r\n */\r\ntemplate< class T >\r\n   T& peek( raft::signal *signal = nullptr );\r\n\r\n/** \r\n * recycle - removes the item at the head of the queue, or \r\n * number of items specified as range.\r\n */\r\nvirtual void recycle( const std::size_t range = 1 );\r\n\r\n/**\r\n * NOTE: there will be a pop_range function, however its exact form might\r\n * soon change, so it is not listed in this summary.\r\n */\r\n```\r\n\r\nTo \"hook\" compute kernels together we need a \"Map\" object, as in the following example (shamelessly taken\r\nfrom the above \"sum\" example):\r\n```c++\r\nint\r\nmain( int argc, char **argv )\r\n{\r\n   Map map;\r\n   auto linked_kernels( map.link( new Generate< std::int64_t >(),\r\n                                  new Sum< std::int64_t,std::int64_t, std::int64_t >(),\r\n                                  \"input_a\" ) );\r\n   map.link( new Generate< std::int64_t >(), &( linked_kernels.dst ), \"input_b\" );\r\n   map.link( &( linked_kernels.dst ), new Print< std::int64_t >() );\r\n   map.exe();\r\n   return( EXIT_SUCCESS );\r\n}\r\n```\r\n\r\nUsing the _link()_ function we can link together compatible combinations of two compute kernels.  Compatible\r\nports will have the same type or will be able to be cast to the same type. The _link()_ function returns a\r\nreference to the functions that are passed as parameters.  This is a convenience so that the programmer can\r\ninstantiate them via the function param with _new_ and use the return values for follow-on _link()_ calls.  This\r\nfunction has various forms as follows:\r\n```c++\r\n/**\r\n * version 1: Kernel's a & b are assumed to have a single output\r\n * and input port respectively, otherwise an exception is thrown.   \r\n */\r\ntemplate < order::spec t = order::in >\r\n      kernel_pair_t link( Kernel *a, Kernel *b );\r\n/**\r\n * version 2: Kernel a has multiple output ports, but one named _a\\_port_, Kernel\r\n * b has a single input port otherwise an exception is thrown.\r\n */\r\ntemplate < order::spec t = order::in > \r\n      kernel_pair_t link( Kernel *a, const std::string  a_port, Kernel *b );\r\n/** \r\n * version 3: Kernel a has a single output port (otherwise an exception is thrown,\r\n * Kernel b has multiple input ports with at least one named _b\\_port_\r\n */\r\ntemplate < order::spec t = order::in >\r\n      kernel_pair_t link( Kernel *a, Kernel *b, const std::string b_port );\r\n/**\r\n * version 4: both kernels have multiple output ports with at least one named _a\\_port_ \r\n * and _b\\_port_, otherwise an exception is thrown.\r\n */\r\ntemplate < order::spec t = order::in >\r\n      kernel_pair_t link( Kernel *a, const std::string a_port, \r\n                          Kernel *b, const std::string b_port );\r\n```\r\n\r\nCalling link has the effect of connecting Kernel *a to Kernel *b through the \r\nspecified port (or the only port if none is specified).  This creates a data \r\nflow graph of kernels and “streams” through which data passes.  Type checking \r\nis also done at this time between ports.  Types are considered compatible if they\r\nare the same or cast-able.  If types are cast-able then the smallest (narrowest) \r\ntype of the two is chosen for the port allocation.  The types are cast \r\ntransparently to the user.  At this point nothing has yet been allocated.  \r\nWhen the _exe()_ function of Map is called, everything is put into \r\nmotion.  The data flow graph stored within Map is analyzed for \r\n“unconnected” edges, which could indicate compute kernels  which have connected \r\nincorrectly, this is in addition to the type checking already \r\nperformed.  Each compute kernel is “mapped” to a thread or process for \r\nexecution, the appropriate type of link is automatically allocated by the \r\nrun-time, finally the scheduler begins executing the data flow graph.  \r\nIn the final version two threads monitor the execution schedule and stream \r\nstatus ensuring streams are properly allocated.  Future versions will \r\nautomatically increase or decrease the parallelism within the data-flow \r\ngraph (dev version will get this first).  \r\n","google":"UA-55176313-1","note":"Don't delete this file! It's used internally to help with page regeneration."}